---
layout: post
title: "Day 32 – A Dual Approach: Redefining Our AI Research Pathways"
date: 2025-07-09
author: Obaloluwa Wojuade
permalink: /day32.html
tags: ["#AIResearch", "#HybridModels", "#ReinforcementLearning", "#TransformerRL", "#DoubleDuelingDQN", "#SAIRI25"]

what_i_learned: |
  Today marked a turning point in our project. After a valuable check-in with our faculty mentor, we realized that to push the boundaries of our research, a single path might not be enough. So, we've decided to adopt a dual approach moving forward.

  On one hand, we’ll continue refining our current Double Dueling DQN model—diving deeper into its reinforcement learning capabilities and optimizing its core structure. On the other hand, we're now exploring a Transformer-based reinforcement learning model, which opens up a whole new set of possibilities. The goal is to compare their strengths and understand which architecture performs better for our specific problem domain.

  We’re actively experimenting with different hyperparameters, architectures, and configurations across both tracks. This dual strategy isn’t just about performance—it's about gaining insight, flexibility, and robustness in how we solve the challenge in front of us.

blockers: |
  No major blockers today, but the shift does come with a new challenge: managing two experimental pipelines in parallel. This means we’ll need to stay organized, split tasks efficiently, and keep our evaluation methods consistent so we can draw meaningful comparisons between both approaches.

reflection: |
  The decision to explore both DQN and Transformer-based RL models feels like the right move at the right time. It reflects how flexible and iterative AI research needs to be. Even though it adds complexity, it also adds depth—and that’s where the potential for real innovation lives.

  I’m especially curious to see how well the Transformer handles sequential patterns in our biochemical data, and whether it can outperform the more traditional RL approach we're already comfortable with. If nothing else, this pivot broadens our understanding and strengthens the final outcome. We're not just trying to build a smart model—we’re trying to build the right one.
---
